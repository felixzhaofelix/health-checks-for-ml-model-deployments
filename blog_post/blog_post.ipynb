{
 "cells": [
  {
   "cell_type": "raw",
   "id": "404ecfa0",
   "metadata": {},
   "source": [
    "Title: Health Checks for ML Model Deployments\n",
    "Date: 2023-01-15 22:00\n",
    "Category: Blog\n",
    "Slug: health-checks-for-ml-model-deployments\n",
    "Authors: Brian Schmidt\n",
    "Summary: Deploying machine learning models in RESTful services is a common way to make the model available for use within a software system. RESTful services are the most common type of service deployed, since they are very simple to build, have wide compatibility, and have lots of tooling available for them. In order to monitor the availability of the service, RESTful APIs often provide health check endpoints which make it easy for an outside system to verify that the service is up and running. A healthcheck endpoint is a simple endpoint that can be called by a process manager to ascertain whether the application is running correctly. In this blog post we'll be working with Kubernetes so we'll focus on the healthchecks supported by Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47800b58",
   "metadata": {},
   "source": [
    "# Health Checks for ML Model Deployments\n",
    "\n",
    "In a [previous blog post](https://www.tekhnoal.com/rest-model-service.html) we showed how to create a RESTful model service for a machine learning model. In this blog post, we'll extend the model service API by adding healthchecks to it.\n",
    "\n",
    "This blog post was written in a Jupyter notebook, some of the code and commands found in it reflect this.\n",
    "\n",
    "All of the code for this blog post is in [this github repository](https://github.com/schmidtbri/health-checks-for-ml-model-deployments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75b477",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deploying machine learning models in RESTful services is a common way to make the model available for use within a software system. In general, RESTful services are the most common type of service deployed, since they are simple to build, have wide compatibility, and have lots of tooling available for them. In order to monitor the availability of the service, RESTful APIs often provide health check endpoints which make it easy for an outside system to verify that the service is up and running. A healthcheck endpoint is a simple endpoint that can be called by a process manager to assertain whether the application is running correctly. In this blog post we'll be working with Kubernetes so we'll focus on the healthchecks supported by Kubernetes. \n",
    "\n",
    "There are several types of healtchcheck endpoints supported by Kubernetes: startup, readiness, and liveness health checks. Startup checks are used to check if an application has started. If the container has a startup check configured on it, Kubernetes will wait until the application has finished starting up before moving on with the process of making the application available to clients. Startup checks are useful for applications that take a while to startup. Startup checks are only called during application startup, once an application has finished starting up the startup check is not called again.\n",
    "\n",
    "Readiness checks are used to check if a container is ready to start accepting requests. Once the application has finished starting up, Kubernetes uses the readiness check to make sure that the application is able to accept requests. Service readiness can change during the service lifecycle so the check is called continuously until the application is stopped.\n",
    "\n",
    "Liveness checks are used to restart a pod if the application is not responding. They are are the simplest type of check to implement in the application because they should always succeed if the server process is running. Liveness checks are useful to detect if the application is in an unsafe state, if the liveness check fails the process manager needs to restart the application to get it out of the unsafe state. Liveness checks are also called continuously while the application is running.\n",
    "\n",
    "In this blog post, we’ll be adding startup, readiness, and liveness checks to a RESTful model service that is hosting a machine learning model. We'll also build a model that requires healthchecks in order to be deployed correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9643453",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "In order to train a model, we first need to have a dataset. We went into Kaggle and found a dataset that contained loan information. To make it easy to download the data, we installed the [kaggle python package](https://pypi.org/project/kaggle/). Then we executed these commands to download the data and unzip it into the data folder in the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca63663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8978bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../data\n",
    "\n",
    "!kaggle datasets download -d ranadeep/credit-risk-dataset -p ../data --unzip\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5439123",
   "metadata": {},
   "source": [
    "Let's look at the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedde42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 67648\r\n",
      "drwxr-xr-x   6 brian  staff       192 Nov 21 14:45 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  27 brian  staff       864 Jan 11 21:23 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 brian  staff      6148 Oct 31 17:30 .DS_Store\r\n",
      "-rw-r--r--   1 brian  staff     20995 Nov 21 13:49 LCDataDictionary.xlsx\r\n",
      "-rw-r--r--   1 brian  staff  34603008 Nov 21 14:58 credit-risk-dataset.zip\r\n",
      "drwxr-xr-x   3 brian  staff        96 Nov 16 09:33 \u001b[34mloan\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ce8bd",
   "metadata": {},
   "source": [
    "Looks like the data is in a .csv file in the /loan folder and the data dictionary is in an excel spreadsheet file.\n",
    "\n",
    "Let's load the data .csv file into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef02df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../data/loan/loan.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fef747a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 887379 entries, 0 to 887378\n",
      "Data columns (total 74 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   id                           887379 non-null  int64  \n",
      " 1   member_id                    887379 non-null  int64  \n",
      " 2   loan_amnt                    887379 non-null  float64\n",
      " 3   funded_amnt                  887379 non-null  float64\n",
      " 4   funded_amnt_inv              887379 non-null  float64\n",
      " 5   term                         887379 non-null  object \n",
      " 6   int_rate                     887379 non-null  float64\n",
      " 7   installment                  887379 non-null  float64\n",
      " 8   grade                        887379 non-null  object \n",
      " 9   sub_grade                    887379 non-null  object \n",
      " 10  emp_title                    835917 non-null  object \n",
      " 11  emp_length                   842554 non-null  object \n",
      " 12  home_ownership               887379 non-null  object \n",
      " 13  annual_inc                   887375 non-null  float64\n",
      " 14  verification_status          887379 non-null  object \n",
      " 15  issue_d                      887379 non-null  object \n",
      " 16  loan_status                  887379 non-null  object \n",
      " 17  pymnt_plan                   887379 non-null  object \n",
      " 18  url                          887379 non-null  object \n",
      " 19  desc                         126028 non-null  object \n",
      " 20  purpose                      887379 non-null  object \n",
      " 21  title                        887227 non-null  object \n",
      " 22  zip_code                     887379 non-null  object \n",
      " 23  addr_state                   887379 non-null  object \n",
      " 24  dti                          887379 non-null  float64\n",
      " 25  delinq_2yrs                  887350 non-null  float64\n",
      " 26  earliest_cr_line             887350 non-null  object \n",
      " 27  inq_last_6mths               887350 non-null  float64\n",
      " 28  mths_since_last_delinq       433067 non-null  float64\n",
      " 29  mths_since_last_record       137053 non-null  float64\n",
      " 30  open_acc                     887350 non-null  float64\n",
      " 31  pub_rec                      887350 non-null  float64\n",
      " 32  revol_bal                    887379 non-null  float64\n",
      " 33  revol_util                   886877 non-null  float64\n",
      " 34  total_acc                    887350 non-null  float64\n",
      " 35  initial_list_status          887379 non-null  object \n",
      " 36  out_prncp                    887379 non-null  float64\n",
      " 37  out_prncp_inv                887379 non-null  float64\n",
      " 38  total_pymnt                  887379 non-null  float64\n",
      " 39  total_pymnt_inv              887379 non-null  float64\n",
      " 40  total_rec_prncp              887379 non-null  float64\n",
      " 41  total_rec_int                887379 non-null  float64\n",
      " 42  total_rec_late_fee           887379 non-null  float64\n",
      " 43  recoveries                   887379 non-null  float64\n",
      " 44  collection_recovery_fee      887379 non-null  float64\n",
      " 45  last_pymnt_d                 869720 non-null  object \n",
      " 46  last_pymnt_amnt              887379 non-null  float64\n",
      " 47  next_pymnt_d                 634408 non-null  object \n",
      " 48  last_credit_pull_d           887326 non-null  object \n",
      " 49  collections_12_mths_ex_med   887234 non-null  float64\n",
      " 50  mths_since_last_major_derog  221703 non-null  float64\n",
      " 51  policy_code                  887379 non-null  float64\n",
      " 52  application_type             887379 non-null  object \n",
      " 53  annual_inc_joint             511 non-null     float64\n",
      " 54  dti_joint                    509 non-null     float64\n",
      " 55  verification_status_joint    511 non-null     object \n",
      " 56  acc_now_delinq               887350 non-null  float64\n",
      " 57  tot_coll_amt                 817103 non-null  float64\n",
      " 58  tot_cur_bal                  817103 non-null  float64\n",
      " 59  open_acc_6m                  21372 non-null   float64\n",
      " 60  open_il_6m                   21372 non-null   float64\n",
      " 61  open_il_12m                  21372 non-null   float64\n",
      " 62  open_il_24m                  21372 non-null   float64\n",
      " 63  mths_since_rcnt_il           20810 non-null   float64\n",
      " 64  total_bal_il                 21372 non-null   float64\n",
      " 65  il_util                      18617 non-null   float64\n",
      " 66  open_rv_12m                  21372 non-null   float64\n",
      " 67  open_rv_24m                  21372 non-null   float64\n",
      " 68  max_bal_bc                   21372 non-null   float64\n",
      " 69  all_util                     21372 non-null   float64\n",
      " 70  total_rev_hi_lim             817103 non-null  float64\n",
      " 71  inq_fi                       21372 non-null   float64\n",
      " 72  total_cu_tl                  21372 non-null   float64\n",
      " 73  inq_last_12m                 21372 non-null   float64\n",
      "dtypes: float64(49), int64(2), object(23)\n",
      "memory usage: 501.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581aa388",
   "metadata": {},
   "source": [
    "We'll be predicting credit risk. Let's select the most promising columns in the dataset so that we wont have to deal with all of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5ae5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\n",
    "    \"annual_inc\", \n",
    "    \"collections_12_mths_ex_med\", \n",
    "    \"delinq_2yrs\", \n",
    "    \"dti\", \n",
    "    \"emp_length\", \n",
    "    \"home_ownership\", \n",
    "    \"acc_now_delinq\", \n",
    "    \"installment\", \n",
    "    \"int_rate\", \n",
    "    \"last_pymnt_amnt\", \n",
    "    \"loan_amnt\", \n",
    "    \"loan_status\", \n",
    "    \"pub_rec\", \n",
    "    \"purpose\", \n",
    "    \"revol_util\", \n",
    "    \"term\", \n",
    "    \"total_pymnt_inv\", \n",
    "    \"verification_status\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8b239",
   "metadata": {},
   "source": [
    "To make the data easier to explore we'll rename the columns to make their names more user friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553ecf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\n",
    "    \"annual_inc\": \"AnnualIncome\", \n",
    "    \"collections_12_mths_ex_med\": \"CollectionsInLast12Months\",\n",
    "    \"delinq_2yrs\": \"DelinquenciesInLast2Years\", \n",
    "    \"dti\": \"DebtToIncomeRatio\", \n",
    "    \"emp_length\": \"EmploymentLength\", \n",
    "    \"home_ownership\": \"HomeOwnership\", \n",
    "    \"acc_now_delinq\": \"NumberOfDelinquentAccounts\", \n",
    "    \"installment\": \"MonthlyInstallmentPayment\",\n",
    "    \"int_rate\": \"InterestRate\", \n",
    "    \"last_pymnt_amnt\": \"LastPaymentAmount\",  \n",
    "    \"loan_amnt\": \"LoanAmount\",\n",
    "    \"loan_status\": \"LoanStatus\", \n",
    "    \"pub_rec\": \"DerogatoryPublicRecordCount\", \n",
    "    \"purpose\": \"LoanPurpose\",\n",
    "    \"revol_util\": \"RevolvingLineUtilizationRate\", \n",
    "    \"term\": \"Term\", \n",
    "    \"total_pymnt_inv\": \"TotalPaymentsToDate\", \n",
    "    \"verification_status\": \"VerificationStatus\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db176189",
   "metadata": {},
   "source": [
    "We'll also build a simple data dictionary with the column descriptions that were downloaded with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "170705e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = {\n",
    "    \"AnnualIncome\": \"The self-reported annual income provided by the borrower during registration.\", \n",
    "    \"CollectionsInLast12Months\": \"Number of collections in 12 months excluding medical collections.\",\n",
    "    \"DelinquenciesInLast2Years\": \"The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years.\", \n",
    "    \"DebtToIncomeRatio\": \"A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\", \n",
    "    \"EmploymentLength\": \"Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\", \n",
    "    \"HomeOwnership\": \"The home ownership status provided by the borrower during registration. Our values are: RENT, OWN, MORTGAGE, OTHER.\",\n",
    "    \"NumberOfDelinquentAccounts\": \"The number of accounts on which the borrower is now delinquent.\", \n",
    "    \"MonthlyInstallmentPayment\": \"The monthly payment owed by the borrower if the loan originates.\", \n",
    "    \"InterestRate\": \"Interest Rate on the loan.\", \n",
    "    \"LastPaymentAmount\": \"Last total payment amount received.\",\n",
    "    \"LoanAmount\": \"The listed amount of the loan applied for by the borrower.\", \n",
    "    \"LoanStatus\": \"Current status of the loan.\",\n",
    "    \"DerogatoryPublicRecordCount\": \"Number of derogatory public records.\",\n",
    "    \"LoanPurpose\": \"A category provided by the borrower for the loan request.\", \n",
    "    \"RevolvingLineUtilizationRate\": \"Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\", \n",
    "    \"Term\": \"The number of payments on the loan. Values are in months and can be either 36 or 60.\",\n",
    "    \"TotalPaymentsToDate\": \"Payments received to date for portion of total amount funded by investors.\", \n",
    "    \"VerificationStatus\": \"Indicates if income was verified.\", \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54596196",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "\n",
    "Now that we have the dataset, we'll start working on training a model. We'll be doing data exploration, data preparation, model training, and model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b725b",
   "metadata": {},
   "source": [
    "### Profiling the Data\n",
    "\n",
    "Profiling the data can tell us a lot about the internal structure of the dataset. To profile the data, we'll use the [pandas_profiling package](https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/).\n",
    "\n",
    "To install the package, we'll execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d8d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas_profiling\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367594a",
   "metadata": {},
   "source": [
    "The profile is built with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0ca24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(data, \n",
    "                        title=\"Credit Risk Analysis Dataset Report\",\n",
    "                        pool_size=4,\n",
    "                        progress_bar=False,\n",
    "                        dataset={\n",
    "                            \"description\": \"Lending Club loan data, complete loan data for all loans issued through the 2007-2015.\"\n",
    "                        },\n",
    "                        variables={\n",
    "                            \"descriptions\": data_dictionary\n",
    "                        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda6a05",
   "metadata": {},
   "source": [
    "We passed the data dictionary we built to the profile, it will be saved in the report generated.\n",
    "\n",
    "Once the report is created, we'll save it to disk as an HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f1c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"../credit_risk_model/model_files/data_exploration_report.html\")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463114ac",
   "metadata": {},
   "source": [
    "Right away the profile will tell us a few key details about the dataset:\n",
    "\n",
    "![Data Overview](data_overview_hcfmlm.png)\n",
    "![Data Overview]({attach}data_overview_hcfmlm.png){ width=100% }\n",
    "\n",
    "The profile also contains a few alerts about the data:\n",
    "\n",
    "![Data Alerts](data_alerts_hcfmlm.png)\n",
    "![Data Alerts]({attach}data_alerts_hcfmlm.png){ width=100% }\n",
    "\n",
    "The profile has a description for each variable. Here's the description for the \"LoanStatus\" variable, which we will use to build the target variable.\n",
    "\n",
    "![Loan Status Variable Description](loan_status_description_hcfmlm.png)\n",
    "![Loan Status Variable Description]({attach}loan_status_description_hcfmlm.png){ width=100% }\n",
    "\n",
    "By using the pandas_profiling package we can avoid writing the most common data analysis code that we write for all datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ce8c0",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "The column that we're interested in is the \"LoanStatus\" column which tells us the current status of the loan. The values in the column are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80dfbab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fully Paid',\n",
       " 'Charged Off',\n",
       " 'Current',\n",
       " 'Default',\n",
       " 'Late (31-120 days)',\n",
       " 'In Grace Period',\n",
       " 'Late (16-30 days)',\n",
       " 'Does not meet the credit policy. Status:Fully Paid',\n",
       " 'Does not meet the credit policy. Status:Charged Off',\n",
       " 'Issued']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data[\"LoanStatus\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d4a76",
   "metadata": {},
   "source": [
    "We'll be using this column to predict how risky a loan is. To do this, we'll need to create a target column that maps the values above into values that represent the riskiness of the loan. To keep things simple we'll simply create two categories for loans:\n",
    "\n",
    "- \"safe\", for all loans that are current, fully paid off, in grace period, or the payment plan has not started yet\n",
    "- \"risky\", for all other loans\n",
    "\n",
    "Now we'll map the values above to the categories we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043bbfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(887379, 19)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"LoanRisk\"] = data[\"LoanStatus\"].replace({\n",
    "    \"Fully Paid\": \"safe\",\n",
    "    \"Charged Off\": \"risky\",\n",
    "    \"Current\": \"safe\",\n",
    "    \"Default\": \"risky\",\n",
    "    \"Late (31-120 days)\": \"risky\",\n",
    "    \"In Grace Period\": \"safe\",\n",
    "    \"Late (16-30 days)\": \"risky\",\n",
    "    \"Does not meet the credit policy. Status:Fully Paid\": \"safe\",\n",
    "    \"Does not meet the credit policy. Status:Charged Off\": \"risky\",\n",
    "    \"Issued\": \"safe\"\n",
    "})\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da8b85",
   "metadata": {},
   "source": [
    "Now we can remove the \"LoanStatus\" column since we created a new target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c53cbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(887379, 18)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(\"LoanStatus\", axis=1)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba322e",
   "metadata": {},
   "source": [
    "Now that have a defined target column, we can move on to fixing some things in the dataset. From the profile we can see that there are several problems with the data that we need to fix.\n",
    "\n",
    "The profile tells us that there are rows with missing data. To simplify the data modeling, we'll drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90149c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841954, 18)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce727f36",
   "metadata": {},
   "source": [
    "All of the rows with missing values are now gone.  \n",
    "\n",
    "The \"AnnualIncome\" column is highly skewed. This is due to some rows which have outlier values, for example the max value for this column is $9,500,000. We'll fix this by removing rows with outlier values in that column. We'll remove the rows with values in this column that are more than three standard deviations from the mean like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e83d2b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(835167, 18)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "data = data[(np.abs(stats.zscore(data[\"AnnualIncome\"])) < 3)]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469ae79",
   "metadata": {},
   "source": [
    "Another column in the dataset that is highly skewed is \"DebtToIncomeRatio\". For example, the maximum value in this column is 9999 which is probably not correct since most of the values in the column have a range between 0 and 100.\n",
    "\n",
    "We'll use the same code to remove the outlier values for DebtToIncomeRatio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13d8836a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(835112, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[(np.abs(stats.zscore(data[\"DebtToIncomeRatio\"])) < 3)]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867167df",
   "metadata": {},
   "source": [
    "The column \"NumberOfDelinquentAccounts\" is highly skewed because of a single record that has a value of 14. We'll remove the outliers by simply filtering out the rows with values above 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5018686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(835111, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data[\"NumberOfDelinquentAccounts\"] <= 6]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b7762",
   "metadata": {},
   "source": [
    "The \"HomeOwnership\" column has several values that stand in for missing data. These values make up a small portion of the dataset, so we'll just remove the rows instead of doing data imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6036ae0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834889, 18)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data[\"HomeOwnership\"] != \"OTHER\"]\n",
    "data = data[data[\"HomeOwnership\"] != \"NONE\"]\n",
    "data = data[data[\"HomeOwnership\"] != \"ANY\"]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49deacf3",
   "metadata": {},
   "source": [
    "Looks like we only lost a few hundred rows from the dataset.\n",
    "\n",
    "The variable \"CollectionsInLast12Months\" is not highly skewed but it contains values that only appear once or very few times. There are very few samples that have a value above 5, these samples are likely not useful so we'll remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb4aabaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834884, 18)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data[\"CollectionsInLast12Months\"] <= 5]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa1b8a",
   "metadata": {},
   "source": [
    "The same is true for the \"DelinquenciesInLast2Years\" and \"DerogatoryPublicRecordCount\" variables. There are very few samples with a value above 10 for these variables. We'll remove those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02d57b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(834415, 18)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data[\"DelinquenciesInLast2Years\"] <= 10]\n",
    "data = data[data[\"DerogatoryPublicRecordCount\"] <= 10]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c83d8",
   "metadata": {},
   "source": [
    "The variable \"RevolvingLineUtilizationRate\" is a percent whose values must be between 0 and 100. There really isn't a way to use more than 100% of your revolving line of credit. However, this variable has values above 100, we'll remove those samples because they're bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29d1949a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831103, 18)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data[\"RevolvingLineUtilizationRate\"] <= 100.0]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20332d9f",
   "metadata": {},
   "source": [
    "### Validating the Data\n",
    "\n",
    "Next, we'll use the [deepchecks package](https://docs.deepchecks.com/stable/getting-started/welcome.html) to do ML specific checks on the data. These checks are for checking for data issues that might affect an ML model.\n",
    "\n",
    "Let's install the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cbb8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepchecks\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3de4e",
   "metadata": {},
   "source": [
    "Before we can run these checks, we need to specify which variables are categorical and numerical, and which variable is the target variable. We'll create lists of variable names for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41f492a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = [\n",
    "    \"EmploymentLength\", \n",
    "    \"HomeOwnership\",\n",
    "    \"LoanPurpose\", \n",
    "    \"VerificationStatus\",\n",
    "    \"Term\"\n",
    "]\n",
    "\n",
    "numerical_variables = [\n",
    "    \"AnnualIncome\", \n",
    "    \"CollectionsInLast12Months\",\n",
    "    \"DelinquenciesInLast2Years\", \n",
    "    \"DebtToIncomeRatio\", \n",
    "    \"NumberOfDelinquentAccounts\", \n",
    "    \"MonthlyInstallmentPayment\", \n",
    "    \"InterestRate\", \n",
    "    \"LastPaymentAmount\",\n",
    "    \"LoanAmount\", \n",
    "    \"DerogatoryPublicRecordCount\",\n",
    "    \"RevolvingLineUtilizationRate\", \n",
    "    \"TotalPaymentsToDate\"\n",
    "]\n",
    "\n",
    "target_variable = \"LoanRisk\"\n",
    "\n",
    "all_variables = categorical_variables + numerical_variables + [target_variable]\n",
    "\n",
    "assert len(all_variables) == len(data.columns), \"A column is missing from the lists of variables.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e048cb",
   "metadata": {},
   "source": [
    "The assert statement at the end of the code above is a simple check that makes sure that we don't forget to include all of the columns in the lists. If we forget to include a column in the list it will stop execution of the notebook with an error message.\n",
    "\n",
    "In order to avoid some errors in the training process, we'll need to change the column type of some of the columns in the pandas dataframe to \"category\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a1ef5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in categorical_variables:\n",
    "    data[column_name] = data[column_name].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f742c28d",
   "metadata": {},
   "source": [
    "Lets start the data validation checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5602bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n",
    "from deepchecks.tabular.suites import data_integrity\n",
    "\n",
    "dataset = Dataset(data, \n",
    "                  cat_features=categorical_variables,  \n",
    "                  label=target_variable)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00edc1f1",
   "metadata": {},
   "source": [
    "The Dataset object contains a reference to the original Dataframe that we've been working with, and also contains the metadata about the columns in the dataframe that is needed to analyze the data.\n",
    "\n",
    "We'll run the checks on the data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e131a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_integrity_suite = data_integrity()\n",
    "\n",
    "suite_result = data_integrity_suite.run(dataset)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbddcde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../credit_risk_model/model_files/deepchecks_data_integrity_results.html'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite_result.save_as_html(\"../credit_risk_model/model_files/deepchecks_data_integrity_results.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3044bfd",
   "metadata": {},
   "source": [
    "The checks done by this suite are geared towards datasets used for machine learning.\n",
    "\n",
    "The results of the data integrity suite look like this:\n",
    "\n",
    "![Data Integrity Suite](data_integrity_suite_hcfmlm.png)\n",
    "![Data Integrity Suite]({attach}data_integrity_suite_hcfmlm.png){ width=100% }\n",
    "\n",
    "The suite contains many checks that execute on the data set, the checks that passed are:\n",
    "\n",
    "- Feature Label Correlation, predictive power score is less than 0.8 for all features.\n",
    "- Single Value in Column, column does not contain only a single value\n",
    "- Special Characters, ratio of samples containing solely special character is less or equal to 0.1%\n",
    "- Mixed Nulls, number of different null types is less or equal to 1\n",
    "- Mixed Data Types, rare data types in column are either more than 10% or less than 1% of the data\n",
    "- Data Duplicates, duplicate data ratio is less or equal to 0%\n",
    "- String Length Out Of Bounds, ratio of string length outliers is less or equal to 0%\n",
    "- Conflicting Labels, ambiguous sample ratio is less or equal to 0%\n",
    "\n",
    "The checks are fully explained in the [deepchecks documentation](https://docs.deepchecks.com/0.9/checks_gallery/tabular.html).\n",
    "\n",
    "For now, we're more interested in the checks that did not pass:\n",
    "\n",
    "![Data Integrity Suite Fail](data_integrity_suite_fail_hcfmlm.png)\n",
    "![Data Integrity Suite Fail]({attach}data_integrity_suite_fail_hcfmlm.png){ width=100% }\n",
    "\n",
    "The two checks that didn't pass are:\n",
    "\n",
    "- Feature-Feature Correlation, not more than 0 pairs are correlated above 0.9\n",
    "- String Mismatch, no string variants\n",
    "\n",
    "The deepchecks package found that the \"LoanAmount\" and \"MonthlyInstallmentPayment\" variables are highly correlated, which makes sense because an increase in loan amount will always cause an increase in payment amount. We can safely drop the MonthlyInstallmentPayment column from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09e7dd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831103, 17)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(\"MonthlyInstallmentPayment\", axis=1)\n",
    "\n",
    "numerical_variables.remove(\"MonthlyInstallmentPayment\")\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e760df8",
   "metadata": {},
   "source": [
    "The deepchecks package also found that the \"EmploymentLength\" variable contains string values that are similar to each other. For example, two levels found in the categorical variable are \"1 year\" and \"< 1 year\". This is a warning that we can ignore because the levels are correctly set.\n",
    "\n",
    "We're now getting closer to a dataset that we can use to train a model. We'll be using deepchecks to do train/test dataset checks and model checks later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69312f1e",
   "metadata": {},
   "source": [
    "### Training a Model\n",
    "\n",
    "To train a model, we'll first create a training and testing set. We'll use 80% of the rows for training and 20% of the rows for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a33d7f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(664983, 17)\n",
      "(166120, 17)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mask = np.random.rand(len(data)) < 0.80\n",
    "training_data = data[mask]\n",
    "testing_data = data[~mask]\n",
    "\n",
    "print(training_data.shape)\n",
    "print(testing_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3920bb9",
   "metadata": {},
   "source": [
    "We will be running a test suite on the newly created training and test suites using deepchecks. The deepchecks package requires that we create two Dataset objects, one for the training set and one for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e5755e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(training_data, \n",
    "                        label=target_variable,\n",
    "                        cat_features=categorical_variables)\n",
    "\n",
    "test_dataset = Dataset(testing_data, \n",
    "                       label=target_variable,\n",
    "                       cat_features=categorical_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c41ac4",
   "metadata": {},
   "source": [
    "Now we can run the train-test validation suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8591a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import train_test_validation\n",
    "\n",
    "validation_suite = train_test_validation()\n",
    "\n",
    "suite_result = validation_suite.run(train_dataset, test_dataset)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e9695",
   "metadata": {},
   "source": [
    "We'll save the results to files for this suite as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8a05c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../credit_risk_model/model_files/deepchecks_train_test_results.html'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite_result.save_as_html(\"../credit_risk_model/model_files/deepchecks_train_test_results.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87fb18",
   "metadata": {},
   "source": [
    "The results of the suite look like this:\n",
    "\n",
    "![Train Test Suite](train_test_suite_hcfmlm.png)\n",
    "![Train Test Suite]({attach}train_test_suite_hcfmlm.png){ width=100% }\n",
    "\n",
    "All of the checks in this suite passed:\n",
    "\n",
    "- Datasets Size Comparison, Test-Train size ratio is greater than 0.01\n",
    "- Category Mismatch Train Test, ratio of samples with a new category is less or equal to 0%\n",
    "- Feature Label Correlation Change, Train-Test features' Predictive Power Score difference is less than 0.2\n",
    "- Feature Label Correlation Change, Train features' Predictive Power Score is less than 0.7\n",
    "- Train Test Feature Drift, categorical drift score < 0.2 and numerical drift score < 0.1\n",
    "- Train Test Label Drift, categorical drift score < 0.2 and numerical drift score < 0.1 for label drift\tLabel's drift score Cramer's V is 0\n",
    "- New Label Train Test, number of new label values is less or equal to 0\n",
    "- String Mismatch Comparison\tNo new variants allowed in test data\n",
    "- Train Test Samples Mix, percentage of test data samples that appear in train data is less or equal to 10%, \n",
    "- Multivariate Drift, drift value is less than 0.25\n",
    "\n",
    "These checks are more fully explained in the [documentation](https://docs.deepchecks.com/0.9/checks_gallery/tabular/train_test_validation/).\n",
    "\n",
    "Now that we have verified the contents of the training and testing sets, we're finally ready to traing a model. To do this, we'll need to create separate dataframes for the predictor and target columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bbec414",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = categorical_variables + numerical_variables\n",
    "\n",
    "X_train = training_data[feature_columns]\n",
    "y_train = training_data[target_variable]\n",
    "\n",
    "X_test = testing_data[feature_columns]\n",
    "y_test = testing_data[target_variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc3eda",
   "metadata": {},
   "source": [
    "We'll be using the [LightGBM package](https://lightgbm.readthedocs.io/en/latest/index.html) to train a GBM model and the [FLAML package](https://microsoft.github.io/FLAML/) for doing automated machine learning. Let's install the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b08d3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "!pip install flaml\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71814f3",
   "metadata": {},
   "source": [
    "Let's train a model using the default hyperparameters to have a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daa062ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 657 ms, total: 13.8 s\n",
      "Wall time: 3.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier()\n",
    "\n",
    "model = model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571a60c",
   "metadata": {},
   "source": [
    "Now let's calculate the classification metrics for this simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a389e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       risky       0.92      0.14      0.24     11456\n",
      "        safe       0.94      1.00      0.97    154664\n",
      "\n",
      "    accuracy                           0.94    166120\n",
      "   macro avg       0.93      0.57      0.61    166120\n",
      "weighted avg       0.94      0.94      0.92    166120\n",
      "\n",
      "CPU times: user 8.96 s, sys: 191 ms, total: 9.15 s\n",
      "Wall time: 6.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbd723",
   "metadata": {},
   "source": [
    "The \"safe\" class has good metrics but the \"risky\" class does not. This is due to the fact that the classes are imbalanced. \n",
    "\n",
    "We'll try to fix this issue by doing automated ML with the FLAML package. The automated hyperparameter search will hopefully find some parameters that can improve the metrics of the \"risky\" class.\n",
    "\n",
    "The settings are:\n",
    "\n",
    "- time_budget: amount of time allowed for the auto ML algorithm to run\n",
    "- metric: the metric that should be maximized by the auto ML alogorithm\n",
    "- estimator_list: the types of estimators that can be used by FLAML, in this case we only want to try LightGBM\n",
    "- task: the type of task that the estimator should be solving\n",
    "- log_file_name: name of the log file output by the auto ML algorithm\n",
    "- seed: the random seed to be used by the auto ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49560e2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "\n",
    "automl = AutoML()\n",
    "\n",
    "settings = {\n",
    "    \"time_budget\": 1200,\n",
    "    \"metric\": \"roc_auc\",\n",
    "    \"estimator_list\": [\"lgbm\"],\n",
    "    \"task\": \"classification\",\n",
    "    \"log_file_name\": \"experiment.log\",\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "automl.fit(X_train=X_train, y_train=y_train, **settings)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4141a51",
   "metadata": {},
   "source": [
    "The hyperparameter search has found an optimal set of hyperparametes using the training set and cross validation. These are the hyperparameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55941d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10707,\n",
       " 'num_leaves': 7,\n",
       " 'min_child_samples': 62,\n",
       " 'learning_rate': 0.24185440044608203,\n",
       " 'log_max_bin': 10,\n",
       " 'colsample_bytree': 0.9914098492087268,\n",
       " 'reg_alpha': 2.551067627605118,\n",
       " 'reg_lambda': 0.0010846951681516895}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312319d6",
   "metadata": {},
   "source": [
    "Let's train a model using the optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4936651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(**automl.best_config)\n",
    "\n",
    "model = model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485bc4b0",
   "metadata": {},
   "source": [
    "Let's get the the classification metrics for the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e8d7de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       risky       0.89      0.46      0.61     11456\n",
      "        safe       0.96      1.00      0.98    154664\n",
      "\n",
      "    accuracy                           0.96    166120\n",
      "   macro avg       0.93      0.73      0.79    166120\n",
      "weighted avg       0.96      0.96      0.95    166120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98504718",
   "metadata": {},
   "source": [
    "### Validating the Model\n",
    "\n",
    "Deepchecks is also able to validate the model with the model_evaluation suite of checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b3f554f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import model_evaluation\n",
    "\n",
    "evaluation_suite = model_evaluation()\n",
    "\n",
    "suite_result = evaluation_suite.run(train_dataset, test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37a7fafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../credit_risk_model/model_files/deepchecks_model_evaluation_results.html'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite_result.save_as_html(\"../credit_risk_model/model_files/deepchecks_model_evaluation_results.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386865d",
   "metadata": {},
   "source": [
    "### Packaging the Model Files\n",
    "\n",
    "Let's serialize the best model to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24b97d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 82.1 ms, total: 1.41 s\n",
      "Wall time: 226 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../credit_risk_model/model_files/model.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"../credit_risk_model/model_files/model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95eb1b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 82824\r\n",
      "drwxr-xr-x  8 brian  staff      256 Jan 15 20:07 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  8 brian  staff      256 Dec 13 11:15 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@ 1 brian  staff     6148 Jan 15 19:09 .DS_Store\r\n",
      "-rw-r--r--  1 brian  staff  9426966 Jan 15 19:12 data_exploration_report.html\r\n",
      "-rw-r--r--  1 brian  staff  7750291 Jan 15 19:15 deepchecks_data_integrity_results.html\r\n",
      "-rw-r--r--@ 1 brian  staff  7964754 Jan 15 20:06 deepchecks_model_evaluation_results.html\r\n",
      "-rw-r--r--  1 brian  staff  7793791 Jan 15 19:17 deepchecks_train_test_results.html\r\n",
      "-rw-r--r--  1 brian  staff  9452707 Jan 15 20:07 model.joblib\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../credit_risk_model/model_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88854b",
   "metadata": {},
   "source": [
    "The serialized model is 9.5 megabytes in size and took 226 milliseconds to write to disk. This is important to note because we will need to deserialize the model later in order to make predictions with it.\n",
    "\n",
    "In the process of training this model, we created a few files. To be able to use these files later, we'll package them up and save them in a location that can be accessed by the prediction code later. We'll be using a .zip file for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e82263b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/brian/Code/health-checks-for-ml-model-deployments/credit_risk_model/model_files/1.zip'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "shutil.make_archive(\"../credit_risk_model/model_files/1\", \"zip\", \"../credit_risk_model/model_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7da67",
   "metadata": {},
   "source": [
    "The command created a .zip file with all of the files in the model_files folder. The name of the folder is \"1.zip\" this is just a simple name that denotes that it is the first model trained for the credit_risk_model package.\n",
    "\n",
    "Now that we have the model files in a .zip file, we can delete the original files from the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a348aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../credit_risk_model/model_files/data_exploration_report.html\n",
    "!rm ../credit_risk_model/model_files/deepchecks_data_integrity_results.html\n",
    "!rm ../credit_risk_model/model_files/deepchecks_train_test_results.html\n",
    "!rm ../credit_risk_model/model_files/deepchecks_model_evaluation_results.html\n",
    "!rm ../credit_risk_model/model_files/model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85f711",
   "metadata": {},
   "source": [
    "This packaging process ensures that all of the results of the model training process end up in one archive that we can use later. All of the data and model check results are packaged along with the serialized model so its easy to review the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d563b0",
   "metadata": {},
   "source": [
    "## Making Predictions with the Model\n",
    "\n",
    "We now have a working model that accepts Pandas dataframes as input and also returns predictions in dataframes. This is useful in the context of model training, but makes integrating the model with other software components a lot more complicated. To make the model easier to use, we'll need to create input and output schemas for the model and also create a wrapper class that provides a consistent interface for the model. \n",
    "\n",
    "We'll create the model's input and output schemas with the [pydantic package](https://pydantic-docs.helpmanual.io/), which is a package used for data validation. By creating the schemas using this package we're able to fully document the inputs that the model accepts and the expected outputs of the model we're going to deploy.\n",
    "\n",
    "To begin, we'll define the allowed values for the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "664463b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class EmploymentLength(str, Enum): \n",
    "    \"\"\"Employment length in years.\"\"\"\n",
    "    less_than_1_year = \"< 1 year\"\n",
    "    one_year = \"1 year\"\n",
    "    two_years = \"2 years\"\n",
    "    three_years = \"3 years\"\n",
    "    four_years = \"4 years\"\n",
    "    five_years = \"5 years\"\n",
    "    six_years = \"6 years\"\n",
    "    seven_years = \"7 years\"\n",
    "    eight_years = \"8 years\"\n",
    "    nine_years = \"9 years\"\n",
    "    ten_years_or_more = \"10+ years\"\n",
    "\n",
    "    \n",
    "class HomeOwnership(str, Enum):\n",
    "    \"\"\"The home ownership status provided by the borrower during registration.\"\"\"\n",
    "    MORTGAGE = \"MORTGAGE\"\n",
    "    RENT = \"RENT\"\n",
    "    OWN = \"OWN\"\n",
    "\n",
    "\n",
    "class LoanPurpose(str, Enum):\n",
    "    \"\"\"A category provided by the borrower for the loan request.\"\"\"\n",
    "    debt_consolidation = \"debt_consolidation\"\n",
    "    credit_card = \"credit_card\"\n",
    "    home_improvement = \"home_improvement\"\n",
    "    other = \"other\"\n",
    "    major_purchase = \"major_purchase\"\n",
    "    small_business = \"small_business\"\n",
    "    car = \"car\"\n",
    "    medical = \"medical\"\n",
    "    moving = \"moving\"\n",
    "    vacation = \"vacation\"\n",
    "    wedding = \"wedding\"\n",
    "    house = \"house\"\n",
    "    renewable_energy = \"renewable_energy\"\n",
    "    educational = \"educational\"\n",
    "    \n",
    "\n",
    "class Term(str, Enum):\n",
    "    \"\"\"The number of payments on the loan.\"\"\"\n",
    "    thirty_six_months = \" 36 months\"\n",
    "    sixty_months = \" 60 months\"\n",
    "\n",
    "\n",
    "class VerificationStatus(str, Enum):\n",
    "    \"\"\"Indicates if income was verified.\"\"\"\n",
    "    source_verified = \"Source Verified\"\n",
    "    verified = \"Verified\"\n",
    "    not_verified = \"Not Verified\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c3cad",
   "metadata": {},
   "source": [
    "The dataset contains 5 categorical variables, so we defined 5 Enum classes that contain the values accepted for these variables. Each enumeration has a key and value, with the value being the value as the model expects to see it. By using enumerated values, we can ensure that the model can only receive values in these inputs that it has previously seen in the training set.\n",
    "\n",
    "Now that we have the categorical variables defined, we can define the input schema for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a21a2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditRiskModelInput(BaseModel):\n",
    "    \"\"\"Inputs for predicting credit risk.\"\"\"\n",
    "    annual_income: int = Field(ge=1896, le=273000, description=\"The self-reported annual income provided by the borrower during registration.\")\n",
    "    collections_in_last_12_months: int = Field(ge=0, le=20, description=\"Number of collections in 12 months excluding medical collections.\")\n",
    "    delinquencies_in_last_2_years: int = Field(ge=0, le=39, description=\"The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years.\")\n",
    "    debt_to_income_ratio: float = Field(ge=0.0, le=42.64, description=\"A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\")\n",
    "    employment_length: EmploymentLength = Field(description=\"Employment length in years.\")\n",
    "    home_ownership: HomeOwnership = Field(description=\"The home ownership status provided by the borrower during registration. Our values are: RENT, OWN, MORTGAGE, OTHER.\")\n",
    "    number_of_delinquent_accounts: int = Field(ge=0, le=6, description=\"The number of accounts on which the borrower is now delinquent.\")\n",
    "    interest_rate: float = Field(ge=5.32, le=28.99, description=\"Interest Rate on the loan.\")\n",
    "    last_payment_amount: float = Field(ge=0.0, le=36475.59, description=\"Last total payment amount received.\")\n",
    "    loan_amount: int = Field(ge=500.0, le=35000.0, description=\"The listed amount of the loan applied for by the borrower.\")\n",
    "    derogatory_public_record_count: int = Field(ge=0.0, le=86.0, description=\"Number of derogatory public records.\")\n",
    "    loan_purpose: LoanPurpose = Field(description=\"A category provided by the borrower for the loan request.\")\n",
    "    revolving_line_utilization_rate: float = Field(ge=0.0, le=892.3, description=\"Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\")\n",
    "    term: Term = Field(description=\"The number of payments on the loan. Values are in months and can be either 36 or 60.\")\n",
    "    total_payments_to_date: float = Field(ge=0.0, le=57777.58, description=\"Payments received to date for portion of total amount funded by investors.\")\n",
    "    verification_status: VerificationStatus = Field(description=\"Indicates if income was verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537f667",
   "metadata": {},
   "source": [
    "The schema is called \"CreditRiskModelInput\" and contains fields for each variable found in the dataset. We're using the Enum classes we defined above for the categorical fields, and we defined fields for all of the numerical variables. Each numerical field has a range of allowed values that matches the range of the numerical variable found in the dataset. Each field also has a description of the variable that helps the user of the model to correctly feed data to the model.\n",
    "\n",
    "The process of creating an input data schema makes the model much more used friendly and exposes information found in the dataset that the model was originally trained on to the user of the model. Doing this also allows us to build documentation for the model service automatically.\n",
    "\n",
    "Now that we have the model's input schema defined, we'll define the output schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7bd1eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditRisk(str, Enum):\n",
    "    \"\"\"Indicates whether or not loan is risky.\"\"\"\n",
    "    safe = \"safe\"\n",
    "    risky = \"risky\"\n",
    "    \n",
    "\n",
    "class CreditRiskModelOutput(BaseModel):\n",
    "    credit_risk: CreditRisk = Field(description=\"Whether or not the loan is risky.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2deb6",
   "metadata": {},
   "source": [
    "The model is a classification model and the output schema simply enumerates the classes that the model can predict. \n",
    "\n",
    "We now have the input and output schemas defined, now we can tie it all together by creating a wrapper class for the model. The [ml_base package](https://pypi.org/project/ml-base/) defines a simple base class for model prediction code that allows us to \"wrap\" the prediction code in a class that follows the MLModel interface. This interface publishes this information about the model:\n",
    "\n",
    "- Qualified Name, a unique identifier for the model\n",
    "- Display Name, a friendly name for the model used in user interfaces\n",
    "- Description, a description for the model\n",
    "- Version, semantic version of the model codebase\n",
    "- Input Schema, an object that describes the model\\'s input data\n",
    "- Output Schema, an object that describes the model\\'s output schema\n",
    "\n",
    "The MLModel interface also dictates that the model class implements two methods:\n",
    "\n",
    "- \\_\\_init\\_\\_, the initialization method which loads any model artifacts needed to make predictions \n",
    "- predict, prediction method that receives model inputs makes a prediction and returns model outputs \n",
    "\n",
    "By using the MLModel base class we'll be able to do more interesting things later with the model. If you'd like to learn more about the ml_base package, [here](https://schmidtbri.github.io/ml-base/basic/) is some documentation about it.\n",
    "\n",
    "To install the ml_base package, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc835b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ml_base\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d9259",
   "metadata": {},
   "source": [
    "We'll define the wrapper class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "180b5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from ml_base import MLModel\n",
    "import zipfile\n",
    "\n",
    "\n",
    "__file__ = os.path.join(os.path.dirname(os.path.realpath(os.path.abspath(''))), \"credit_risk_model\", \"prediction\", \"model.py\")\n",
    "\n",
    "class CreditRiskModel(MLModel):\n",
    "    \"\"\"Prediction logic for the Credit Risk Model.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def display_name(self) -> str:\n",
    "        \"\"\"Return display name of model.\"\"\"\n",
    "        return \"Credit Risk Model\"\n",
    "\n",
    "    @property\n",
    "    def qualified_name(self) -> str:\n",
    "        \"\"\"Return qualified name of model.\"\"\"\n",
    "        return \"credit_risk_model\"\n",
    "\n",
    "    @property\n",
    "    def description(self) -> str:\n",
    "        \"\"\"Return description of model.\"\"\"\n",
    "        return \"Model to predict the credit risk of a loan.\"\n",
    "\n",
    "    @property\n",
    "    def version(self) -> str:\n",
    "        \"\"\"Return version of model.\"\"\"\n",
    "        return \"0.1.0\"\n",
    "\n",
    "    @property\n",
    "    def input_schema(self):\n",
    "        \"\"\"Return input schema of model.\"\"\"\n",
    "        return CreditRiskModelInput\n",
    "\n",
    "    @property\n",
    "    def output_schema(self):\n",
    "        \"\"\"Return output schema of model.\"\"\"\n",
    "        return CreditRiskModelOutput\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Class constructor that loads and deserializes the model parameters.\"\"\"\n",
    "        dir_path = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n",
    "        file_path = os.path.join(dir_path, \"model_files\", \"1.zip\")\n",
    "\n",
    "        with zipfile.ZipFile(file_path) as zf:\n",
    "            if \"model.joblib\" not in zf.namelist():\n",
    "                raise ValueError(\"Could not find model file in zip file.\")\n",
    "            model_file = zf.open(\"model.joblib\")\n",
    "            self._model = joblib.load(model_file)\n",
    "    \n",
    "    def predict(self, data: CreditRiskModelInput) -> CreditRiskModelOutput:\n",
    "        \"\"\"Make a prediction with the model.\n",
    "        \n",
    "        Params:\n",
    "            data: Data for making a prediction with the model.\n",
    "        \n",
    "        Returns:\n",
    "            The result of the prediction.\n",
    "        \n",
    "        \"\"\"\n",
    "        if type(data) is not CreditRiskModelInput:\n",
    "            raise ValueError(\"Input must be of type 'CreditRisk'\")\n",
    "            \n",
    "        X = pd.DataFrame([[\n",
    "            data.employment_length.value,\n",
    "            data.home_ownership.value,\n",
    "            data.loan_purpose.value,\n",
    "            data.verification_status.value,\n",
    "            data.term.value,\n",
    "            data.annual_income,\n",
    "            data.collections_in_last_12_months,\n",
    "            data.delinquencies_in_last_2_years,\n",
    "            data.debt_to_income_ratio,\n",
    "            data.number_of_delinquent_accounts,\n",
    "            data.interest_rate,\n",
    "            data.last_payment_amount,\n",
    "            data.loan_amount,\n",
    "            data.derogatory_public_record_count,\n",
    "            data.revolving_line_utilization_rate,\n",
    "            data.total_payments_to_date,\n",
    "        ]],\n",
    "            columns=[\n",
    "                \"EmploymentLength\", \n",
    "                \"HomeOwnership\", \n",
    "                \"LoanPurpose\",\n",
    "                \"VerificationStatus\", \n",
    "                \"Term\", \n",
    "                \"AnnualIncome\",\n",
    "                \"CollectionsInLast12Months\", \n",
    "                \"DelinquenciesInLast2Years\",\n",
    "                \"DebtToIncomeRatio\",\n",
    "                \"NumberOfDelinquentAccounts\", \n",
    "                \"InterestRate\",\n",
    "                \"LastPaymentAmount\", \n",
    "                \"LoanAmount\", \n",
    "                \"DerogatoryPublicRecordCount\",\n",
    "                \"RevolvingLineUtilizationRate\",\n",
    "                \"TotalPaymentsToDate\"\n",
    "            ])\n",
    "        \n",
    "        categorical_variables = [\"EmploymentLength\", \n",
    "                                 \"HomeOwnership\", \n",
    "                                 \"LoanPurpose\",\n",
    "                                 \"VerificationStatus\", \n",
    "                                 \"Term\"]\n",
    "        \n",
    "        for column_name in categorical_variables:\n",
    "            X[column_name] = X[column_name].astype(\"category\")\n",
    "\n",
    "        y_hat = self._model.predict(X)[0]\n",
    "        \n",
    "        return CreditRiskModelOutput(credit_risk=CreditRisk[y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475887d",
   "metadata": {},
   "source": [
    "We can make a prediction with the model by first building a CreditRiskModelInput object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf6cccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = CreditRiskModelInput(\n",
    "    annual_income=273000, \n",
    "    collections_in_last_12_months=20, \n",
    "    delinquencies_in_last_2_years=39, \n",
    "    debt_to_income_ratio=42.64, \n",
    "    employment_length=EmploymentLength.less_than_1_year, \n",
    "    home_ownership=HomeOwnership.MORTGAGE, \n",
    "    number_of_delinquent_accounts=6, \n",
    "    interest_rate=28.99, \n",
    "    last_payment_amount=36475.59, \n",
    "    loan_amount=35000,  \n",
    "    derogatory_public_record_count=86, \n",
    "    loan_purpose=LoanPurpose.debt_consolidation, \n",
    "    revolving_line_utilization_rate=892.3, \n",
    "    term=Term.thirty_six_months, \n",
    "    total_payments_to_date=57777.58, \n",
    "    verification_status=VerificationStatus.source_verified \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f91ec",
   "metadata": {},
   "source": [
    "Next, we'll instantiate the model class we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92d193fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 372 ms, sys: 29.2 ms, total: 401 ms\n",
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = CreditRiskModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c26fe",
   "metadata": {},
   "source": [
    "Notice that the model object took 113 milliseconds to be instantiated. This is because the model parameters take a lot of disk space and take a while to load from the hard drive. This is something that we'll need to deal with later.\n",
    "\n",
    "We'll use the CreditRiskModelInput instance to make a prediction like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a22702a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreditRiskModelOutput(credit_risk=<CreditRisk.safe: 'safe'>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(model_input)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d86a5",
   "metadata": {},
   "source": [
    "The model predicted that the loan is \"safe\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe43d5",
   "metadata": {},
   "source": [
    "## Creating a RESTful Service\n",
    "\n",
    "Now that we have a model, we can deploy it in a service that allows clients to make predictions. To do this, we won't need to write any extra code, we can leverage the [rest_model_service package](https://pypi.org/project/rest-model-service/) to provide the RESTful API for the service. You can learn more about the package in [this blog post](https://www.tekhnoal.com/rest-model-service.html).\n",
    "\n",
    "To install the package, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "841fc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rest_model_service\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5878b63",
   "metadata": {},
   "source": [
    "To create a service for our model, all that is needed is that we add a YAML configuration file to the project. The configuration file looks like this:\n",
    "\n",
    "```yaml\n",
    "service_title: Credit Risk Model Service\n",
    "description: \"Service hosting the Credit Risk Model.\"\n",
    "version: \"0.1.0\"\n",
    "models:\n",
    "  - qualified_name: credit_risk_model\n",
    "    class_path: credit_risk_model.prediction.model.CreditRiskModel\n",
    "    create_endpoint: true\n",
    "```\n",
    "\n",
    "At the root of the YAML, the \"service_title\" field is the name of the service as it will appear in the documentation. The \"description\" and \"version\"fields will also be used to create the service documentation.\n",
    "\n",
    "The models field is an array that contains the details of the models we would like to deploy in the service. The \"qualified_name\" field is the name we gave to the model. The \"class_path\" field points at the MLModel class that implements the model's prediction logic, in this case it is pointing to the class we built earlier in this blog post. The \"create_endpoint\" field tells the service to create an endpoint for the model.\n",
    "\n",
    "Using the configuration file, we can create an OpenAPI specification file for the model service by executing these commands:\n",
    "\n",
    "```bash\n",
    "export PYTHONPATH=./\n",
    "generate_openapi --configuration_file=./configuration/rest_configuration.yaml --output_file=\"service_contract.yaml\"\n",
    "```\n",
    "\n",
    "The service_contract.yaml file is generated and contains the OpenAPI specification that was generated for the model service. The specification contains a description of the model's endpoint. The model's input and output schemas are automatically extracted and added to the specification. The OpenAPI specification file generated can be found at the root of the github repository in the file named service_contract.yaml\n",
    "\n",
    "To run the service locally, execute these commands:\n",
    "\n",
    "```bash\n",
    "export REST_CONFIG=./configuration/rest_configuration.yaml\n",
    "uvicorn rest_model_service.main:app\n",
    "```\n",
    "\n",
    "The service process starts up and can be accessed in a web browser at http://127.0.0.1:8000. The service renders the OpenAPI specification as a webpage that looks like this:\n",
    "\n",
    "![]Service Documentation{ width=100% }\n",
    "\n",
    "\n",
    "![Service Documentation](service_documentation_hcfmlm.png)\n",
    "![Service Documentation]({attach}service_documentation_hcfmlm.png){ width=100% }\n",
    "\n",
    "By using the MLModel base class provided by the ml_base package and the REST service framework provided by the rest_model_service package we're able to quickly stand up a service to host the model.\n",
    "\n",
    "We can make a prediction using the model running in the service with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9547cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"credit_risk\":\"safe\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'POST' \\\n",
    "  'http://127.0.0.1:8000/api/models/credit_risk_model/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{ \\\n",
    "      \\\"annual_income\\\": 273000, \\\n",
    "      \\\"collections_in_last_12_months\\\": 20, \\\n",
    "      \\\"delinquencies_in_last_2_years\\\": 39, \\\n",
    "      \\\"debt_to_income_ratio\\\": 42.64, \\\n",
    "      \\\"employment_length\\\": \\\"< 1 year\\\", \\\n",
    "      \\\"home_ownership\\\": \\\"MORTGAGE\\\", \\\n",
    "      \\\"number_of_delinquent_accounts\\\": 6, \\\n",
    "      \\\"interest_rate\\\": 28.99, \\\n",
    "      \\\"last_payment_amount\\\": 36475.59, \\\n",
    "      \\\"loan_amount\\\": 35000,  \\\n",
    "      \\\"derogatory_public_record_count\\\": 86, \\\n",
    "      \\\"loan_purpose\\\": \\\"debt_consolidation\\\", \\\n",
    "      \\\"revolving_line_utilization_rate\\\": 892.3, \\\n",
    "      \\\"term\\\": \\\" 36 months\\\", \\\n",
    "      \\\"total_payments_to_date\\\": 57777.58, \\\n",
    "      \\\"verification_status\\\": \\\"Source Verified\\\" \\\n",
    "}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49506a9d",
   "metadata": {},
   "source": [
    "The model returned a prediction of \"safe\" for the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f49ff",
   "metadata": {},
   "source": [
    "## Understanding Health Checks\n",
    "\n",
    "The service is able exposes health information about itself through health endpoints. The health endpoints are:\n",
    "\n",
    "- /api/health: indicates whether the service process is running. This endpoint will return a 200 status once the service has started.\n",
    "- /api/health/ready: indicates whether the service is ready to respond to requests. This endpoint will return a 200 status only if all the models and decorators have finished being instantiated without errors.\n",
    "- /api/health/startup: indicates whether the service is started. This endpoint will return a 200 status only if all the models and decorators have finished being instantiated without errors.\n",
    "\n",
    "These endpoints are important for our use case because our model takes a while to load and become ready to be served over the API. The service will not be ready to serve traffic for a while, so the readiness and startup checks will fail until the models are ready.\n",
    "\n",
    "The service is running so we'll try out each endpoint with a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6639b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"health_status\":\"HEALTHY\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://127.0.0.1:8000/api/health' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96f9c1",
   "metadata": {},
   "source": [
    "The health endpoint returned a status of \"HEALTHY\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b36affe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"readiness_status\":\"ACCEPTING_TRAFFIC\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://127.0.0.1:8000/api/health/ready' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c1fcb",
   "metadata": {},
   "source": [
    "The readiness status endpoint returned a status of \"ACCEPTING_TRAFFIC\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f01e626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"startup_status\":\"STARTED\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://127.0.0.1:8000/api/health/startup' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39848a9d",
   "metadata": {},
   "source": [
    "The startup status endpoint returned a status of \"STARTED\".\n",
    "\n",
    "During normal operation, the health endpoints are not very interesting. However, in special situations they are very useful. For example, if a model takes a long time to start up, the startup check endpoint will not return a 200 status response until each model is initiated and ready to make predictions. The readiness endpoint will also not return a 200 status until the model is ready. We'll use the healtcheck endpoints to integrated with Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9212a",
   "metadata": {},
   "source": [
    "## Creating a Docker Image\n",
    "\n",
    "Now that we have a working model and model service, we'll need to deploy it somewhere. We'll start by deploying the service locally using Docker.\n",
    "\n",
    "Let's create a docker image and run it locally. The docker image is generated using instructions in the Dockerfile:\n",
    "\n",
    "```Dockerfile\n",
    "# syntax=docker/dockerfile:1\n",
    "FROM python:3.9-slim\n",
    "\n",
    "ARG BUILD_DATE\n",
    "\n",
    "LABEL org.opencontainers.image.title=\"Health Checks for ML Models\"\n",
    "LABEL org.opencontainers.image.description=\"Health checks for ML models.\"\n",
    "LABEL org.opencontainers.image.created=$BUILD_DATE\n",
    "LABEL org.opencontainers.image.authors=\"6666331+schmidtbri@users.noreply.github.com\"\n",
    "LABEL org.opencontainers.image.source=\"https://github.com/schmidtbri/health-checks-for-ml-models\"\n",
    "LABEL org.opencontainers.image.version=\"0.1.0\"\n",
    "LABEL org.opencontainers.image.licenses=\"MIT License\"\n",
    "LABEL org.opencontainers.image.base.name=\"python:3.9-slim\"\n",
    "\n",
    "WORKDIR /service\n",
    "\n",
    "ARG USERNAME=service-user\n",
    "ARG USER_UID=10000\n",
    "ARG USER_GID=10000\n",
    "\n",
    "# install packages\n",
    "RUN apt-get update \\\n",
    "    && apt-get install --assume-yes --no-install-recommends sudo \\\n",
    "    && apt-get install --assume-yes --no-install-recommends git \\\n",
    "    && apt-get install -y --no-install-recommends apt-utils \\\n",
    "    && apt-get install -y --no-install-recommends libgomp1 \\\n",
    "    && apt-get clean \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# create a user\n",
    "RUN groupadd --gid $USER_GID $USERNAME \\\n",
    "    && useradd --uid $USER_UID --gid $USER_GID -m $USERNAME \\\n",
    "    && echo $USERNAME ALL=\\(root\\) NOPASSWD:ALL > /etc/sudoers.d/$USERNAME \\\n",
    "    && chmod 0440 /etc/sudoers.d/$USERNAME\n",
    "\n",
    "# installing dependencies\n",
    "COPY ./service_requirements.txt ./service_requirements.txt\n",
    "RUN pip install -r service_requirements.txt\n",
    "\n",
    "# copying code and license\n",
    "COPY ./credit_risk_model ./credit_risk_model\n",
    "COPY ./LICENSE ./LICENSE\n",
    "\n",
    "USER $USERNAME\n",
    "\n",
    "CMD [\"uvicorn\", \"rest_model_service.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "The Dockerfile is used by this docker command to create a docker image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbaea04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t credit_risk_model_service:0.1.0 ../\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52124b",
   "metadata": {},
   "source": [
    "To make sure everything worked as expected, we'll look through the docker images in our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2096a5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_risk_model_service         0.1.0     fc3c3c747e2b   4 seconds ago   614MB\r\n"
     ]
    }
   ],
   "source": [
    "!docker image ls | grep credit_risk_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59730471",
   "metadata": {},
   "source": [
    "The credit_risk_model_service image is listed. Next, we'll start the image to see if the service is working correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a6f9d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706ff17d8db159568989d8a74221b8bc3bbcb52074ca61da1ee4015297035dc6\r\n"
     ]
    }
   ],
   "source": [
    "!docker run -d \\\n",
    "    -p 8000:8000 \\\n",
    "    -e REST_CONFIG=./configuration/rest_configuration.yaml \\\n",
    "    -v $(pwd)/../configuration:/service/configuration \\\n",
    "    --name credit_risk_model_service \\\n",
    "    credit_risk_model_service:0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b97ee",
   "metadata": {},
   "source": [
    "To make sure the server process started up correctly, we'll look at the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b800999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1]\r\n",
      "INFO:     Waiting for application startup.\r\n",
      "INFO:     Application startup complete.\r\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\n"
     ]
    }
   ],
   "source": [
    "!docker logs credit_risk_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4820c0d",
   "metadata": {},
   "source": [
    "The logs look good and the service is up and running.\n",
    "\n",
    "The service should be accessible on port 8000 of localhost, so we'll try to make a prediction using the curl command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35f8e867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"credit_risk\":\"safe\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'POST' \\\n",
    "  'http://127.0.0.1:8000/api/models/credit_risk_model/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{ \\\n",
    "      \\\"annual_income\\\": 273000, \\\n",
    "      \\\"collections_in_last_12_months\\\": 20, \\\n",
    "      \\\"delinquencies_in_last_2_years\\\": 39, \\\n",
    "      \\\"debt_to_income_ratio\\\": 42.64, \\\n",
    "      \\\"employment_length\\\": \\\"< 1 year\\\", \\\n",
    "      \\\"home_ownership\\\": \\\"MORTGAGE\\\", \\\n",
    "      \\\"number_of_delinquent_accounts\\\": 6, \\\n",
    "      \\\"interest_rate\\\": 28.99, \\\n",
    "      \\\"last_payment_amount\\\": 36475.59, \\\n",
    "      \\\"loan_amount\\\": 35000,  \\\n",
    "      \\\"derogatory_public_record_count\\\": 86, \\\n",
    "      \\\"loan_purpose\\\": \\\"debt_consolidation\\\", \\\n",
    "      \\\"revolving_line_utilization_rate\\\": 892.3, \\\n",
    "      \\\"term\\\": \\\" 36 months\\\", \\\n",
    "      \\\"total_payments_to_date\\\": 57777.58, \\\n",
    "      \\\"verification_status\\\": \\\"Source Verified\\\" \\\n",
    "}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723bd9d2",
   "metadata": {},
   "source": [
    "The model predicted that the loan is safe.\n",
    "\n",
    "We're done with the docker container, so we'll shut down the model service container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "428c3665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit_risk_model_service\n",
      "credit_risk_model_service\n"
     ]
    }
   ],
   "source": [
    "!docker kill credit_risk_model_service\n",
    "!docker rm credit_risk_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890496f",
   "metadata": {},
   "source": [
    "## Creating a Kubernetes Cluster\n",
    "\n",
    "To show the system in action, we’ll deploy the service to a Kubernetes cluster. A local cluster can be easily started by using [minikube](https://minikube.sigs.k8s.io/docs/). Installation instructions can be found [here](https://minikube.sigs.k8s.io/docs/start/).\n",
    "\n",
    "To start the minikube cluster execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4e4da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😄  minikube v1.28.0 on Darwin 13.0.1\n",
      "✨  Using the docker driver based on existing profile\n",
      "👍  Starting control plane node minikube in cluster minikube\n",
      "🚜  Pulling base image ...\n",
      "🔄  Restarting existing docker container for \"minikube\" ...\n",
      "🐳  Preparing Kubernetes v1.25.3 on Docker 20.10.20 ...\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\u001b[K\n",
      "🔎  Verifying Kubernetes components...\n",
      "    ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0\n",
      "    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5\n",
      "    ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8\n",
      "💡  Some dashboard features require the metrics-server addon. To enable all features please run:\n",
      "\n",
      "\tminikube addons enable metrics-server\t\n",
      "\n",
      "\n",
      "🌟  Enabled addons: storage-provisioner, default-storageclass, dashboard\n",
      "🏄  Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n"
     ]
    }
   ],
   "source": [
    "!minikube start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c814e0",
   "metadata": {},
   "source": [
    "Let's view all of the pods running in the minikube cluster to make sure we can connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c89d279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE              NAME                                        READY   STATUS    RESTARTS         AGE\r\n",
      "kube-system            coredns-565d847f94-2v6l9                    0/1     Running   10 (3d20h ago)   11d\r\n",
      "kube-system            etcd-minikube                               0/1     Running   10 (3d20h ago)   11d\r\n",
      "kube-system            kube-apiserver-minikube                     0/1     Running   10 (3d20h ago)   11d\r\n",
      "kube-system            kube-controller-manager-minikube            0/1     Running   10 (25s ago)     11d\r\n",
      "kube-system            kube-proxy-ztbgd                            1/1     Running   10 (25s ago)     11d\r\n",
      "kube-system            kube-scheduler-minikube                     0/1     Running   10 (25s ago)     11d\r\n",
      "kube-system            storage-provisioner                         1/1     Running   18 (25s ago)     11d\r\n",
      "kubernetes-dashboard   dashboard-metrics-scraper-b74747df5-x559p   1/1     Running   9 (25s ago)      11d\r\n",
      "kubernetes-dashboard   kubernetes-dashboard-57bbdc5f89-9jvln       1/1     Running   14 (3d20h ago)   11d\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2c9e8",
   "metadata": {},
   "source": [
    "The pods running the kubernetes dashboard and other cluster services appear in the kube-system and kubernetes-dashboard namespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c8404",
   "metadata": {},
   "source": [
    "## Creating a Kubernetes Namespace\n",
    "\n",
    "Now that we have a cluster and are connected to it, we'll create a namespace to hold the resources for our model deployment. The resource definition is in the kubernetes/namespace.yaml file. To apply the manifest to the cluster, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14082800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/model-services created\r\n",
      "resourcequota/model-services-resource-quota created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f ../kubernetes/namespace.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7666829",
   "metadata": {},
   "source": [
    "The namespace was created, alongside with a ResourceQuota which limits the amount of resources that can be taken by objects within the namespace.\n",
    "\n",
    "To take a look at the namespaces, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "745c5dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   STATUS   AGE\r\n",
      "default                Active   11d\r\n",
      "kube-node-lease        Active   11d\r\n",
      "kube-public            Active   11d\r\n",
      "kube-system            Active   11d\r\n",
      "kubernetes-dashboard   Active   11d\r\n",
      "model-services         Active   0s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928daa5",
   "metadata": {},
   "source": [
    "The new namespace appears in the listing along with other namespaces created by default by the system. To use the new namespace for the rest of the operations, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b3935bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context \"minikube\" modified.\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl config set-context --current --namespace=model-services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0d186",
   "metadata": {},
   "source": [
    "## Creating a Kubernetes Deployment and Service\n",
    "\n",
    "The model service is deployed by using Kubernetes resources. These are:\n",
    "\n",
    "- Model Service ConfigMap: a set of configuration options, in this case it is a simple YAML file that will be loaded into the running container as a volume mount. This resource allows us to change the configuration of the model service without having to modify the Docker image. The configuration file will overwrite the configuration files that were included with the Docker image.\n",
    "- Deployment: a declarative way to manage a set of pods, the model service pods are managed through the Deployment. This deployment includes the model service as well as the OPA service running as a sidecar container.\n",
    "- Service: a way to expose a set of pods in a Deployment, the model services is made available to the outside world through the Service.\n",
    "\n",
    "The Deployment resource will be created with some special options that can leverage the health endpoints of the model service. These options look like this:\n",
    "\n",
    "```yaml\n",
    "livenessProbe:\n",
    "  httpGet:\n",
    "    scheme: HTTP\n",
    "    path: /api/health\n",
    "    port: 8000\n",
    "  initialDelaySeconds: 0\n",
    "  periodSeconds: 5\n",
    "  timeoutSeconds: 2\n",
    "  failureThreshold: 5\n",
    "  successThreshold: 1\n",
    "readinessProbe:\n",
    "  httpGet:\n",
    "    scheme: HTTP\n",
    "    path: /api/health/ready\n",
    "    port: 8000\n",
    "  initialDelaySeconds: 0\n",
    "  periodSeconds: 5\n",
    "  timeoutSeconds: 2\n",
    "  failureThreshold: 5\n",
    "  successThreshold: 1\n",
    "startupProbe:\n",
    "  httpGet:\n",
    "    scheme: HTTP\n",
    "    path: /api/health/startup\n",
    "    port: 8000\n",
    "  initialDelaySeconds: 0\n",
    "  periodSeconds: 5\n",
    "  timeoutSeconds: 2\n",
    "  failureThreshold: 5\n",
    "  successThreshold: 1\n",
    "```\n",
    "  \n",
    "This is not the complete YAML file, the full Deployment is defined in the ./kubernetes/model_service.yaml file.\n",
    "\n",
    "The model service container has options defined for each type of healthcheck. Each type of healthcheck is configured in the same way. The options are:\n",
    "\n",
    "- initialDelaySeconds: This option tells Kubernetes how long to wait after container startup to start calling the healthcheck.\n",
    "- periodSeconds: This option tells how often to call the healthcheck endpoint.\n",
    "- timeoutSeconds: This option tell how long to wait for a response from the service before failing the healthcheck.\n",
    "- failureThreshold: This option tells how many times the healthcheck must fail before Kubernetes labels the container as unhealthy and restarts the pod.\n",
    "- successThreshold: This option tells how many times the healthcheck must succeed before Kubernetes labels the container as healthy.\n",
    "\n",
    "We decided to have Kubernetes check 5 times before labelling the container as unhealthy, with a period of 5 seconds. This means that the service has 25 seconds for the model to finish loading. This is a value we know would work with this model, based on our timing measurements above.\n",
    "\n",
    "We're almost ready to start the model service, but before starting it we'll need to send the docker image from the local docker daemon to the minikube image cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31a9e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "!minikube image load credit_risk_model_service:0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310eb56",
   "metadata": {},
   "source": [
    "We can view the images in the minikube cache with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "136d5f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker.io/library/credit_risk_model_service:0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!minikube image ls | grep credit_risk_model_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881f11a",
   "metadata": {},
   "source": [
    "The model service resources are created within the Kubernetes cluster with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af8d4254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/model-service-configuration created\n",
      "deployment.apps/credit-risk-model-deployment created\n",
      "service/credit-risk-model-service created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../kubernetes/model_service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0b64e",
   "metadata": {},
   "source": [
    "Lets view the Deployment to see if it is available yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0cc0705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           READY   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "credit-risk-model-deployment   0/2     2            0           2s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13efe338",
   "metadata": {},
   "source": [
    "Looks like the replicas are not ready yet. Let's wait a bit and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "abc9626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           READY   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "credit-risk-model-deployment   2/2     2            2           23s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b57e7f",
   "metadata": {},
   "source": [
    "After the model service finished loading the model, it switched to the \"READY\" and \"STARTED\" state, which made the replicas available to serve traffic.\n",
    "\n",
    "To get an idea of how the service went through the startup process, let's look a the service logs. Let's get the names of the pods that are running the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d2a3894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                            READY   STATUS    RESTARTS   AGE\r\n",
      "credit-risk-model-deployment-55654498f4-2bw9k   1/1     Running   0          29s\r\n",
      "credit-risk-model-deployment-55654498f4-rxznw   1/1     Running   0          29s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef0e65",
   "metadata": {},
   "source": [
    "Using one of the pod names, we'll get the logs from Kubernetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3659e2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     172.17.0.1:57232 - \"GET /api/health/startup HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     172.17.0.1:49828 - \"GET /api/health/startup HTTP/1.1\" 503 Service Unavailable\n",
      "INFO:     172.17.0.1:49844 - \"GET /api/health/startup HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:49858 - \"GET /api/health/ready HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:40210 - \"GET /api/health/ready HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:40212 - \"GET /api/health HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:40224 - \"GET /api/health/ready HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:40236 - \"GET /api/health HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:58908 - \"GET /api/health/ready HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:58910 - \"GET /api/health HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:58926 - \"GET /api/health/ready HTTP/1.1\" 200 OK\n",
      "INFO:     172.17.0.1:58942 - \"GET /api/health HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs credit-risk-model-deployment-55654498f4-2bw9k -c credit-risk-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c593eb",
   "metadata": {},
   "source": [
    "Looks like the process started up correctly and then the /api/health/startup endpoint was called three times, succeeding in the last request. Right after the startup check succeeded, the /api/health/ready endpoint was called, and it immediately succeeded. Right after that, the /api/health endpoint was called and it also succeeded. This startup process ensured that the model service was not accessed by clients before it finished loading the models into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a25a0",
   "metadata": {},
   "source": [
    "To access the model service, we created a Kubernetes service. The service details look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6df114c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\r\n",
      "credit-risk-model-service   NodePort   10.110.134.65   <none>        80:31004/TCP   68s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ce3fb",
   "metadata": {},
   "source": [
    "Minikube can expose the service on a local port, but we need to run a proxy process. The proxy is started like this:\n",
    "\n",
    "```bash\n",
    "minikube service credit-risk-model-service --url -n model-services\n",
    "```\n",
    "\n",
    "The command outputs this URL:\n",
    "\n",
    "http://127.0.0.1:59091\n",
    "\n",
    "The command must keep running to keep the tunnel open to the running model service in the minikube cluster.\n",
    "\n",
    "We can send a request to the model service through the local endpoint like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "602b33bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"credit_risk\":\"safe\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'POST' \\\n",
    "  'http://127.0.0.1:59091/api/models/credit_risk_model/prediction' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d \"{ \\\n",
    "      \\\"annual_income\\\": 273000, \\\n",
    "      \\\"collections_in_last_12_months\\\": 20, \\\n",
    "      \\\"delinquencies_in_last_2_years\\\": 39, \\\n",
    "      \\\"debt_to_income_ratio\\\": 42.64, \\\n",
    "      \\\"employment_length\\\": \\\"< 1 year\\\", \\\n",
    "      \\\"home_ownership\\\": \\\"MORTGAGE\\\", \\\n",
    "      \\\"number_of_delinquent_accounts\\\": 6, \\\n",
    "      \\\"interest_rate\\\": 28.99, \\\n",
    "      \\\"last_payment_amount\\\": 36475.59, \\\n",
    "      \\\"loan_amount\\\": 35000,  \\\n",
    "      \\\"derogatory_public_record_count\\\": 86, \\\n",
    "      \\\"loan_purpose\\\": \\\"debt_consolidation\\\", \\\n",
    "      \\\"revolving_line_utilization_rate\\\": 892.3, \\\n",
    "      \\\"term\\\": \\\" 36 months\\\", \\\n",
    "      \\\"total_payments_to_date\\\": 57777.58, \\\n",
    "      \\\"verification_status\\\": \\\"Source Verified\\\" \\\n",
    "}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935cac14",
   "metadata": {},
   "source": [
    "The healthcheck endpoints of the model service are also available to clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1919c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"health_status\":\"HEALTHY\"}"
     ]
    }
   ],
   "source": [
    "!curl -X 'GET' \\\n",
    "  'http://127.0.0.1:59091/api/health' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676dd605",
   "metadata": {},
   "source": [
    "## Deleting the Resources\n",
    "\n",
    "We're done working with the Kubernetes resources, so we will delete them and shut down the cluster.\n",
    "\n",
    "To delete the model service pods, execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "21b4dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap \"model-service-configuration\" deleted\r\n",
      "deployment.apps \"credit-risk-model-deployment\" deleted\r\n",
      "service \"credit-risk-model-service\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f ../kubernetes/model_service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308cb141",
   "metadata": {},
   "source": [
    "To delete the model-services namespace, delete this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e2a3d040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace \"model-services\" deleted\r\n",
      "resourcequota \"model-services-resource-quota\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f ../kubernetes/namespace.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655cfb15",
   "metadata": {},
   "source": [
    "To shut down the Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "57e96e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✋  Stopping node \"minikube\"  ...\n",
      "🛑  Powering off \"minikube\" via SSH ...\n",
      "🛑  1 node stopped.\n"
     ]
    }
   ],
   "source": [
    "!minikube stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e5603",
   "metadata": {},
   "source": [
    "## Closing\n",
    "\n",
    "In this blog post we showed how to deal with a common issue that arises when a large model is deployed. When the model parameters take a long time to load, the model service needs to make sure that no clients are depending on it to provide predictions until it is finished starting up. We accomplished this on the Kubernetes platform by adding health check endpoints to the model service and configuring Kubernetes to check on the endpoints. By doing this we are able to guarantee that the model service will only become available to clients once it has finished starting up. \n",
    "\n",
    "In order to build the healthchecks, the model did not need to change at all. We were able to build the logic into the model service package, which means that the model prediction logic did not have to change to deal with this requirement. We were able to isolate a deployment concern from the model that we were trying to deploy. This also means that we can reuse the model to make predictions in other contexts and not have this extra logic being carried along with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
